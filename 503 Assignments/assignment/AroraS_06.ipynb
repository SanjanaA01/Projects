{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AroraS_06.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ox4vuiHONvD7"
      },
      "source": [
        "**Question 6**\n",
        "Student : Arora, Sanjana (V00966221)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZj9CMRluJKh"
      },
      "source": [
        "** Q6.1. Predicting poverty level using decision regressor **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ldr-9DJPuYZM"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbDKl6qVONxs"
      },
      "source": [
        "Please download election_clean file provided along with the zipped folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIC31d0PziUJ"
      },
      "source": [
        "df = pd.read_csv('elections_clean.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJCGjzkABMxT",
        "outputId": "8ddbf9f9-4ae2-4448-978b-59e22abdefe9"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'votes', 'UnemploymentRate2015', 'MedHHInc2014',\n",
              "       'PerCapitaInc', 'PovertyAllAgesPct2014', 'Deep_Pov_All', 'Population',\n",
              "       'Area in square miles - Total area', 'PopDensity', 'TOT_MALE_rate',\n",
              "       'TOT_FEMALE_rate', 'voter_turnout_rate', 'Democrat', 'State', 'County',\n",
              "       'Education', 'Religion', 'Old', 'Young', 'Adult', 'EthnicMale',\n",
              "       'EthnicFemale'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5IbObokzvN6"
      },
      "source": [
        "# Creating subset of dataframes capturing desired features for analysis\n",
        "subset = df[['Education', 'Religion', 'EthnicMale', 'EthnicFemale','PerCapitaInc']].copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACtU4kuLEVWT"
      },
      "source": [
        "# Transforming the categorical features - percapita feature is excluded as it is continuous\n",
        "subset_transform = pd.get_dummies(subset[['Education', 'Religion', 'EthnicMale', 'EthnicFemale']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEgHrhB-FRaT"
      },
      "source": [
        "# making the combined dataset including the percapita\n",
        "X = pd.concat([subset_transform, df['PerCapitaInc']], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSyEnGfAB4Pn"
      },
      "source": [
        "from sklearn import tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQHmCCWXCKf2"
      },
      "source": [
        "# Target Feature\n",
        "Y = df['PovertyAllAgesPct2014']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5snfbwFBDnCG"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.3, shuffle= True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGZq5Y5TD0Wc"
      },
      "source": [
        "tree_reg = tree.DecisionTreeRegressor(random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuRTjJuBD4dL"
      },
      "source": [
        "tree_reg = tree_reg.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYZRbUlDF-aQ"
      },
      "source": [
        "Y_pred = tree_reg.predict(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pj95L3RRGKUA",
        "outputId": "8bff12f3-8b46-40eb-fd8c-bac8f738c2f1"
      },
      "source": [
        "print('The training Mean Absolute Error is:', mean_absolute_error(y_train,Y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The training Mean Absolute Error is: 0.0007914584279872785\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43R-hg_NGEqg"
      },
      "source": [
        "Y_pred1 = tree_reg.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOz4NhEMGXCE",
        "outputId": "7822a8d5-8267-4502-9b29-65dc4d162147"
      },
      "source": [
        "print('The testing Mean Absolute Error is:', mean_absolute_error(y_test,Y_pred1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The testing Mean Absolute Error is: 0.03526271186440678\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lO57LoJX-FKv"
      },
      "source": [
        "Q6.2 **5 Fold Cross Validation from Scratch**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYPkT60748ZD"
      },
      "source": [
        "# Splitting the dataset into training and testing dataset\n",
        "def train_validation_split(dataset):\n",
        "    validation_split = .3\n",
        "    dataset_size = len(dataset)\n",
        "    split = int(np.floor(validation_split * dataset_size))\n",
        "    training_data = dataset.iloc[split:].reset_index(drop=True)\n",
        "    validation_data = dataset.iloc[:split].reset_index(drop=True)\n",
        "    return training_data,validation_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TG60aRro5AAT"
      },
      "source": [
        "# Subset to be considered for analysis\n",
        "subset2 = pd.concat([subset_transform, df['PerCapitaInc'], df['PovertyAllAgesPct2014']], axis=1)\n",
        "training_data,testing_data = train_validation_split(subset2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7n7QiM838rf7",
        "outputId": "66226c2c-a78e-4dee-faa9-f041907376b5"
      },
      "source": [
        "training_data.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Education_BachelorOrHigher', 'Education_College',\n",
              "       'Education_LessthanDiploma', 'Education_OnlyDiploma', 'Religion_Amish',\n",
              "       'Religion_Catholic', 'Religion_Christian Generic', 'Religion_Mormon',\n",
              "       'Religion_Other', 'Religion_Other Misc', 'EthnicMale_ASIAN_MALE_rate',\n",
              "       'EthnicMale_BLACK_MALE_rate', 'EthnicMale_NATIVE_AMERICAN_MALE_rate',\n",
              "       'EthnicMale_WHITE_MALE_rate', 'EthnicFemale_BLACK_FEMALE_rate',\n",
              "       'EthnicFemale_NATIVE_AMERICAN_FEMALE_rate',\n",
              "       'EthnicFemale_WHITE_FEMALE_rate', 'PerCapitaInc',\n",
              "       'PovertyAllAgesPct2014'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neQ4PnJXu2eF"
      },
      "source": [
        "# function for creating indices for n folds of dataset\n",
        "def generate_folds(df, n):\n",
        "    \n",
        "    #Getting a randmly shuffled index array of the rows of dataframe  \n",
        "    split=np.random.choice(a = int(df.shape[0]), size=int(df.shape[0])) \n",
        "    \n",
        "    #Splitting the index array into n arrays\n",
        "    #Storing the random index of the n folds dataframes\n",
        "    five_split = np.array_split(split, n)\n",
        "    # creating empty list to store dataframes\n",
        "    fold =[]\n",
        "    #Creating the n folds\n",
        "    for i in range(n):\n",
        "    \n",
        "      df_fold = df.iloc[five_split[i]] \n",
        "      fold.append(df_fold)\n",
        "    return fold\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9GbKg1rHuVq"
      },
      "source": [
        "Function for implementing five fold cross-validation and training the regression tree and evaluating the error metrics. The function takes the following inputs:\n",
        "1. Training Data\n",
        "2. Target Name\n",
        "3. The depth of the regression trees"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhsV5_Obdj4x"
      },
      "source": [
        "def Five_FoldCV_And_ModelEvaluation(training_data,Target_Name, depth_level, n_folds):\n",
        "    \n",
        "    #Creating the Five Fold Dataframes from the list\n",
        "    Z=generate_folds(training_data,n_folds)\n",
        "    df1 = pd.DataFrame(Z[1])\n",
        "    df2 = pd.DataFrame(Z[2])\n",
        "    df3 = pd.DataFrame(Z[3])\n",
        "    df4 = pd.DataFrame(Z[4])\n",
        "    df5 = pd.DataFrame(Z[0])\n",
        "    \n",
        "    from sklearn.tree import DecisionTreeRegressor\n",
        "   \n",
        "    # list capturing the errors at each fold\n",
        "    error_list=[]\n",
        "    \n",
        "    for i in range(1,6):\n",
        "        \n",
        "        if(i==1):\n",
        "            # When i = 1 , making the fifth set as the test set\n",
        "            # And the rest of the sets are merged to be used for training\n",
        "            f5 = df5\n",
        "            \n",
        "            #Merging the rest of the dataframes\n",
        "            Tmerge=pd.concat([df1,df2,df3,df4],ignore_index=True)\n",
        "            \n",
        "            #Creating target vector from the merged dataframe\n",
        "            train_y= Tmerge[Target_Name]\n",
        "            #Feature sets from merged dataframe\n",
        "            train_X= Tmerge.drop(columns=Target_Name,axis=1)\n",
        "            \n",
        "            #Creating target vector from the test dataframe\n",
        "            test_y = f5[Target_Name]\n",
        "            #Feature sets from test dataframe\n",
        "            test_X = f5.drop(columns=Target_Name,axis=1)\n",
        "            \n",
        "            #Model training and evaluation\n",
        "            model_1=DecisionTreeRegressor(max_depth=depth_level)\n",
        "            model_1.fit(train_X,train_y)\n",
        "            predictions_1 = model_1.predict(test_X)\n",
        "            mae_1 = mean_absolute_error(test_y,predictions_1)\n",
        "            \n",
        "            #Storing the Error result in the array\n",
        "            error_list.append(mae_1)\n",
        "            \n",
        "        elif(i==2):\n",
        "            # When i = 1 , making the fourth set as the test set\n",
        "            # And the rest of the sets are merged to be used for training\n",
        "            f4=df4\n",
        "            #Merging the rest of the dataframes\n",
        "            Tmerge=pd.concat([df1,df2,df3,df5],ignore_index=True)\n",
        "            \n",
        "            #Creating target vector from the merged dataframe\n",
        "            train_y= Tmerge[Target_Name]\n",
        "            #Feature sets from merged dataframe\n",
        "            train_X= Tmerge.drop(columns=Target_Name,axis=1)\n",
        "            \n",
        "            #Creating target vector from the test dataframe\n",
        "            test_y = f4[Target_Name]\n",
        "            #Feature sets from test dataframe\n",
        "            test_X = f4.drop(columns=Target_Name,axis=1)\n",
        "            \n",
        "            #Model training and evaluation\n",
        "            model_2=DecisionTreeRegressor(max_depth=depth_level)\n",
        "            model_2.fit(train_X,train_y)\n",
        "            predictions_2 = model_2.predict(test_X)\n",
        "            mae_2 = mean_absolute_error(test_y,predictions_2)\n",
        "            \n",
        "            #Storing the Error result in the array\n",
        "            error_list.append(mae_2)\n",
        "            \n",
        "        elif(i==3):\n",
        "            # When i = 1 , making the third set as the test set\n",
        "            # And the rest of the sets are merged to be used for training\n",
        "            f3=df3\n",
        "            #Merging the rest of the dataframes\n",
        "            Tmerge=pd.concat([df1,df2,df4,df5],ignore_index=True)\n",
        "            \n",
        "            #Creating target vector from the merged dataframe\n",
        "            train_y= Tmerge[Target_Name]\n",
        "            #Feature sets from merged dataframe\n",
        "            train_X= Tmerge.drop(columns=Target_Name,axis=1)\n",
        "            \n",
        "           #Creating target vector from the test dataframe\n",
        "            test_y = f3[Target_Name]\n",
        "            #Feature sets from test dataframe\n",
        "            test_X = f3.drop(columns=Target_Name,axis=1)\n",
        "            \n",
        "            #Model training and evaluation\n",
        "            model_3=DecisionTreeRegressor(max_depth=depth_level)\n",
        "            model_3.fit(train_X,train_y)\n",
        "            predictions_3 = model_3.predict(test_X)\n",
        "            mae_3 = mean_absolute_error(test_y,predictions_3)\n",
        "            \n",
        "            #Storing the Error result in the array\n",
        "            error_list.append(mae_3)\n",
        "        \n",
        "        elif(i==4):\n",
        "            # # When i = 1 , making the second set as the test set\n",
        "            # And the rest of the sets are merged to be used for training\n",
        "            f2=df2\n",
        "            #Merging the rest of the dataframes\n",
        "            fold_merge=pd.concat([df1,df3,df4,df5],ignore_index=True)\n",
        "            \n",
        "            #Creating target vector from the merged dataframe\n",
        "            train_y= Tmerge[Target_Name]\n",
        "           #Feature sets from test dataframe\n",
        "            train_X= Tmerge.drop(columns=Target_Name,axis=1)\n",
        "            \n",
        "            #Creating target vector from the test dataframe\n",
        "            test_y = f2[Target_Name]\n",
        "            #Creating X_test from test fold\n",
        "            test_X = f2.drop(columns=Target_Name,axis=1)\n",
        "            \n",
        "            #Model training and evaluation\n",
        "            model_4=DecisionTreeRegressor(max_depth=depth_level)\n",
        "            model_4.fit(train_X,train_y)\n",
        "            predictions_4 = model_4.predict(test_X)\n",
        "            mae_4 = mean_absolute_error(test_y,predictions_4)\n",
        "            \n",
        "            #Storing the Error result in the array\n",
        "            error_list.append(mae_4)\n",
        "    \n",
        "        elif(i==5):\n",
        "            # # When i = 1 , making the first set as the test set\n",
        "            # And the rest of the sets are merged to be used for training\n",
        "            f1=df1\n",
        "           #Merging the rest of the dataframes\n",
        "            fold_merge=pd.concat([df2,df3,df4,df5],ignore_index=True)\n",
        "            \n",
        "            #Creating target vector from the merged dataframe\n",
        "            train_y= Tmerge[Target_Name]\n",
        "            #Feature sets from merged dataframe\n",
        "            train_X= Tmerge.drop(columns=Target_Name,axis=1)\n",
        "            \n",
        "            #Creating target vector from the test dataframe\n",
        "            test_y = f1[Target_Name]\n",
        "            #Feature sets from test dataframe\n",
        "            test_X = f1.drop(columns=Target_Name,axis=1)\n",
        "            \n",
        "            #Model training and evaluation\n",
        "            model_5=DecisionTreeRegressor(max_depth=depth_level)\n",
        "            model_5.fit(train_X,train_y)\n",
        "            predictions_5 = model_5.predict(test_X)\n",
        "            mae_5 = mean_absolute_error(test_y,predictions_5)\n",
        "            \n",
        "            #Storing the Error result in the array\n",
        "            error_list.append(mae_5)\n",
        "    \n",
        "    #calculating the final error by averaging all the errors\n",
        "    finalError=sum(error_list)/len(error_list)\n",
        "    return finalError, error_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HI-0wEkgWZH",
        "outputId": "b5e84a36-b148-48f9-d018-19e9dd269fd3"
      },
      "source": [
        "# Calculating the metrics by changing the depth of the trees\n",
        "# Considering depth = 5\n",
        "result,error_list = Five_FoldCV_And_ModelEvaluation(training_data,'PovertyAllAgesPct2014',5,5)\n",
        "print(result,error_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.02364809832545115 [0.024954431108903153, 0.02333241227138217, 0.024147091000610545, 0.023687591297824807, 0.022118965948535074]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmwmMWfuHhmp",
        "outputId": "3bbe59d9-afb4-4e5c-999a-a7ba1680a23e"
      },
      "source": [
        "# Considering depth = 3\n",
        "result,error_list = Five_FoldCV_And_ModelEvaluation(training_data,'PovertyAllAgesPct2014',3,5)\n",
        "print(result,error_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.026664994304767077 [0.026147782892587783, 0.027361346558500626, 0.02659574023863196, 0.025696225200869158, 0.02752387663324586]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLu0X0C2Hlkw",
        "outputId": "32c6ce94-1c14-411f-d507-745727636cf7"
      },
      "source": [
        "# considering depth = 7\n",
        "result,error_list = Five_FoldCV_And_ModelEvaluation(training_data,'PovertyAllAgesPct2014',7,5)\n",
        "print(result,error_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.020545218735925387 [0.02080871553798112, 0.02226819678700516, 0.02312959277208091, 0.016873431387443783, 0.01964615719511597]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpu2_UHhGdPn"
      },
      "source": [
        "From the above mean absolute error results on the testing sets using each of the different depth of regression trees, we observe that the MAE average error for testing data is the least for the regression tree with length = 7. Therefore, out of the three depths of the tree considered, the regression tree with depth 7 is the best.  "
      ]
    }
  ]
}