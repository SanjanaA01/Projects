{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RankingDocuments_Inverted_Index.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SanjanaA01/Projects/blob/main/RankingDocuments_Inverted_Index.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUgtbVlwZrJ_"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init(\"spark-2.4.4-bin-hadoop2.7\")# SPARK_HOME\n",
        "\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ka4IPeo7b9SE",
        "outputId": "c6236b23-c4e3-4110-c68c-17de04cb76d2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsWUBAs7cdqw",
        "outputId": "857551d8-1909-4792-b37a-7d62120253f6"
      },
      "source": [
        "!rm -rf input_docs\n",
        "!cp /content/drive/MyDrive/input_docs_sample.zip .\n",
        "!unzip input_docs.zip > /dev/null\n",
        "!ls input_docs/ | wc -l"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unzip:  cannot find or open input_docs.zip, input_docs.zip.zip or input_docs.zip.ZIP.\n",
            "ls: cannot access 'input_docs/': No such file or directory\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjVsitwadVch",
        "outputId": "2b45647a-5854-4c5d-c5e4-a1c354dc70fc"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bJ-7I5pdbnD"
      },
      "source": [
        "**Create an RDD from a text file**\n",
        "\n",
        "Each line of the text file becomes an element of the RDD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "s36LJ548dpmw",
        "outputId": "085076e6-3f59-4e0d-9d39-2c9de01ef905"
      },
      "source": [
        "# wholeTextFiles generates an RDD of pair values, \n",
        "# where the key is the full path of each file, the value is the content of each file\n",
        "#input = sc.wholeTextFiles(\"/content/drive/My\\ Drive/input_docs\");\n",
        "input = sc.wholeTextFiles(\"input_docs\");\n",
        "\n",
        "# Now we strip the prefix of filenames and leave only the basename. \n",
        "# e.g. 'file:/content/drive/My Drive/Colab Notebooks/data_spark/input_docs/3.html'\n",
        "# becomes '3.html' \n",
        "import os\n",
        "\n",
        "#(did,text)\n",
        "input2 = input.map(lambda x: (int(os.path.basename(x[0]).split(\".\")[0]), x[1]))\n",
        "\n",
        "print(input2.take(1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-6b0634081834>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0minput2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mglobal\u001b[0m \u001b[0;36mprint\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36minput2.take\u001b[0m \u001b[0;34m= <bound method RDD.take of PythonRDD[53] at RDD at PythonRDD.scala:53>\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.4-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self=PythonRDD[53] at RDD at PythonRDD.scala:53, num=1)\u001b[0m\n\u001b[1;32m   1325\u001b[0m         \"\"\"\n\u001b[1;32m   1326\u001b[0m         \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m         \u001b[0mtotalParts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mtotalParts\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mself.getNumPartitions\u001b[0m \u001b[0;34m= <bound method PipelinedRDD.getNumPartitions of PythonRDD[53] at RDD at PythonRDD.scala:53>\u001b[0m\n\u001b[1;32m   1328\u001b[0m         \u001b[0mpartsScanned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.4-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mgetNumPartitions\u001b[0;34m(self=PythonRDD[53] at RDD at PythonRDD.scala:53)\u001b[0m\n\u001b[1;32m   2515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2516\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetNumPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2517\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prev_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mself._prev_jrdd.partitions.size\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\n\u001b[1;32m   2518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2519\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mspark-2.4.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self=<py4j.java_gateway.JavaMember object>, *args=())\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m        \u001b[0;36manswer\u001b[0m \u001b[0;34m= 'xro356'\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mself.gateway_client\u001b[0m \u001b[0;34m= <py4j.java_gateway.GatewayClient object at 0x7f4febf50950>\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mself.target_id\u001b[0m \u001b[0;34m= 'o354'\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mself.name\u001b[0m \u001b[0;34m= 'partitions'\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mspark-2.4.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer='xro356', gateway_client=<py4j.java_gateway.GatewayClient object>, target_id='o354', name='partitions')\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m        \u001b[0;36mglobal\u001b[0m \u001b[0;36mformat\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mtarget_id\u001b[0m \u001b[0;34m= 'o354'\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mname\u001b[0m \u001b[0;34m= 'partitions'\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mvalue\u001b[0m \u001b[0;34m= JavaObject id=o356\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o354.partitions.\n: org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: file:/content/input_docs\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:323)\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:265)\n\tat org.apache.spark.input.WholeTextFileInputFormat.setMinPartitions(WholeTextFileInputFormat.scala:52)\n\tat org.apache.spark.rdd.WholeTextFileRDD.getPartitions(WholeTextFileRDD.scala:54)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.api.java.JavaRDDLike$class.partitions(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoXyZeIthagn"
      },
      "source": [
        "nltk.download('punkt')\n",
        "from bs4 import BeautifulSoup\n",
        "import collections\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "stem = PorterStemmer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76IA3G1EfZ5l"
      },
      "source": [
        "# Doc to wordlist function\n",
        "# The output will be a list of tuples such as \n",
        "# (\"apple\", (3,10,10/20)), \n",
        "# where 3 is docid, \n",
        "# 10 is frequency of \"apple\" in this doc, \n",
        "# 20 is maxf in in this doc.\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def dw(docid, htmltext):\n",
        "\n",
        "  htmltext = htmltext.lower() #convert into lowercase\n",
        "  cleantext = BeautifulSoup(htmltext).get_text()\n",
        "  \n",
        "  cleantext = re.sub(\"\\d+\", \" \", cleantext) #remove digits\n",
        "  cleantext = re.sub('[^A-Za-z0-9]+', ' ', cleantext) #remove special characters\n",
        "  cleantext = re.sub(\"\\s+\", \" \", cleantext) #remove extra spaces\n",
        "  \n",
        "  cleantext = word_tokenize(cleantext)\n",
        "  cleantext = [word for word in cleantext if word not in stopwords.words('english')]\n",
        "  \n",
        "  output = [];\n",
        "  counter = Counter(cleantext)\n",
        "  maxf = Counter(cleantext).most_common(1)[0][1]\n",
        "  for ele in counter:\n",
        "    tuple1 = (ele);\n",
        "    tuple2 = (int(docid),counter[ele],counter[ele]/maxf)\n",
        "    tuple3 = (tuple1, tuple2)\n",
        "    output.append(tuple3);\n",
        "  return(output)\n",
        "\n",
        "word_docid_freq_tf = input2.flatMap(lambda x: dw(x[0],x[1]))\n",
        "print(word_docid_freq_tf.take(2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrHzuJcoxQ97"
      },
      "source": [
        "%xmode Verbose\n",
        "#combining all the words and documents ids frequency\n",
        "word_list_freq_tf = word_docid_freq_tf.map(lambda x: (x[0],[x[1]])).reduceByKey(lambda x,y: x+y)\n",
        "# Now create an RDD as follows \n",
        "# (word, [(did1,freq1,tf1), (did2,freq2,tf2), ...])\n",
        "word_postinglist_freq_tf = sc.parallelize(word_list_freq_tf.collect())\n",
        "print(word_postinglist_freq_tf.count())\n",
        "print(word_postinglist_freq_tf.map(lambda x : (x[0], list(x[1]))).take(1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNd_P1qi27qo"
      },
      "source": [
        "# idf = 1/len(postinglist_tf)\n",
        "%xmode Verbose\n",
        "%time\n",
        "import math\n",
        "from decimal import Decimal\n",
        "with_tfidf = []\n",
        "for row in word_postinglist_freq_tf.collect():\n",
        "  idf = 1/len(row[1]) #present in number of documents\n",
        "  backto_tuple = []\n",
        "  print(row[1])\n",
        "  for eachrow in row[1]:\n",
        "    list_row = list(eachrow) #convert to list\n",
        "    # list_row[2] = float(format(float(list_row[2]*idf), '.5f')) #tf*idf\n",
        "    tfidf_val = Decimal((list_row[2]*idf)) #tf*idf\n",
        "    list_row[2] =\"{:.18f}\".format(float(tfidf_val))\n",
        "    backto_tuple.append(tuple(list_row))\n",
        "\n",
        "  eachtuple = (row[0], backto_tuple)\n",
        "  with_tfidf.append(eachtuple);\n",
        "# print(with_tfidf[0])\n",
        "# creating a RDD\n",
        "word_postinglist_freq_tfidf = sc.parallelize(with_tfidf)\n",
        "\n",
        "print(word_postinglist_freq_tfidf.take(2))\n",
        "print(word_postinglist_freq_tfidf.count(2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqc7sZ7z1DYH"
      },
      "source": [
        "\n",
        "**Calculating Magnitude**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQBv_2NH32gg"
      },
      "source": [
        "# Now, we would like to obtain the magnitude of each doc.\n",
        "# First, produce (did, (freq,tfidf)) for each word of doc did; \n",
        "# We do don't need the word itself, just its (freq,tfidf). \n",
        "# Then, do reduceByKey on these tuples and obtain maxfreq and \n",
        "# magnitude (squared) for each document. \n",
        "%xmode Verbose\n",
        "import math\n",
        "from decimal import Decimal\n",
        "\n",
        "convert_to_list = word_postinglist_freq_tfidf.collect() #make a list\n",
        "convert_to_list = list(map(list, convert_to_list)) \n",
        "new_list = []\n",
        "for val in convert_to_list: \n",
        "  for row in val[1]:\n",
        "    row = list(row)\n",
        "    # print(format(Decimal.from_float(0.1), '.17'))\n",
        "    # squareval = float(format((row[2]*row[2]), '.15'))\n",
        "    # print(type(row[2]))\n",
        "    magnitude = Decimal((float(row[2])**2))\n",
        "    magnitude =\"{:.18f}\".format(float(magnitude))\n",
        "    row = (row[0],(row[1],magnitude))\n",
        "  new_list.append(row)\n",
        "\n",
        "# RDD of (did,(freq,tfidf)) tuples\n",
        "# creating a RDD\n",
        "did_freq_tfidfsq_rdd = sc.parallelize(new_list)\n",
        "\n",
        "print(did_freq_tfidfsq_rdd.take(2))\n",
        "new_list1 = did_freq_tfidfsq_rdd.reduceByKey(lambda x,y: (max(x[0],y[0]),(float(x[1])+float(y[1]))))\n",
        "\n",
        "# Produce (did,(maxf,magnitudesq))\n",
        "# creating a RDD\n",
        "doc_maxf_mag = sc.parallelize(new_list1.collect())\n",
        "\n",
        "#print(doc_maxf_mag.count())\n",
        "print(doc_maxf_mag.take(2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpWD8pvv4IA2"
      },
      "source": [
        "!rm -rf inv_idx\n",
        "word_postinglist_freq_tfidf.saveAsTextFile(\"inv_idx\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0lr0KzP4N7A"
      },
      "source": [
        "!rm -rf doc_mag\n",
        "doc_maxf_mag.saveAsTextFile(\"doc_mag\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFOjSI8k4TJl"
      },
      "source": [
        "!ls -lrt inv_idx\n",
        "!head inv_idx/part-00001\n",
        "!wc -l inv_idx/part-00000\n",
        "!wc -l inv_idx/part-00001\n",
        "!cat inv_idx/part-00000 inv_idx/part-00001 > /content/drive/My\\ Drive/inv_idx.txt\n",
        "!wc -l /content/drive/My\\ Drive/inv_idx.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90sFBzj34fd3"
      },
      "source": [
        "!ls -lrt doc_mag\n",
        "!head doc_mag/part-00000\n",
        "!wc -l doc_mag/part-00000\n",
        "!wc -l doc_mag/part-00001\n",
        "!cat doc_mag/part-00000 doc_mag/part-00001 > /content/drive/My\\ Drive/doc_mag.txt\n",
        "!wc -l /content/drive/My\\ Drive/doc_mag.txt"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}